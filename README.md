## Дипломная работа по профессии "DevOps-инженер с нуля"
### Исполнитель: Давлат Файзиев
### Дипломный руководитель: Булат Замилов
---
Дипломное задание находится по [ссылке](https://github.com/netology-code/devops-diplom-yandexcloud)  

### Подготовка к выполнению:
При выполении дипломной работы использовались следующие приложения:  
| Наименование       | Версия       |
|--------------------|--------------|
| Ubuntu на WSL      | 22.04        | 
| Visual Studio Code | 1.96.4       | 
| Terraform          | v1.8.4       | 
| Kubespray          | release-2.27 | 
| Kubernetes         | v1.31.4      |
| Docker             | ver. 27.3.1  |
| Helm               | v.3.16.3     |

### Создание облачной инфраструктуры
1. Управление инфраструктурой будет осуществляться сервисным аккаунтом с правами "editor", стейт файл будет храниться S3 bucket.
Для создания аккаунта и бакета необходимо перейти в папку S3_bucket_SA и выполнить команды  `terraform init` и `terraform apply`.
![Скриншот 1](img/1.png)   
В результате выполнения кода создается сервисный аккаунт и S3 bucket.  
Чувствительные данные аккаунта сохранаются в файл `.auth-key.json`, данные доступа к бакету в файл `.credentials`.

2. Далее подготовим инфраструктуру для Kubernetes кластера. Инфраструктура будет состоять из 3-х ВМ в разных подсетях и в разных зонах доступности.
Характеристики всех ВМ будут одинаковыми:
- vCPU - 2
- RAM - 4 Гб
- disk - 20 Гб  
ВМ будут создаваться с использованием модуля `compute_instance`.  На ВМ будет создан пользователь и передан его публичный ключ при помощи cloud-config.  
Для поднятия инфраструктуры необходимо перейти в папку infra и выполнить команды  `terraform init` и `terraform apply`.  
При выполнении кода terraform будут использоваться данные аккаунта и бакета из пункта 1.  
![Скриншот 2](img/2.png)   
В результате выполнения кода создается сеть, подсети, ВМ и файл hosts.ini со следующим содержимым для создания Kubernetes кластера.  
![Скриншот 3](img/3.png)   

### Создание Kubernetes кластера
Для создания Kubernetes кластера используется Kubespray.  
В директорию ansible выполним установку Kubespray в соответсвии с документацией. Переключимся на ветку release-2.27.  
Скопируем папку `inventory/sample` с ее содержимым в папку `inventory/mycluster`, также скопируем туда файл hosts.ini.  
В переменную `supplementary_addresses_in_ssl_keys` в файле `inventory/mycluster/group_vars/k8s_cluster/k8s-cluster.yml` добавим внешний ip адрес master-nod-ы для добавления в ssl ключ и возможности внешнего подключения к кластеру.  
Для запуска плейбука необходимо, перейти в папку kubespray и выполнить команду:  
 `ansible-playbook -i inventory/mycluster/hosts.ini -u davlat -b -v --private-key=~/.ssh/yandxcld cluster.yml`  
![Скриншот 4](img/4.png)   
 Скопируем файл конфигурации на master-nod-е из `/etc/kubernetes/admin.conf` в файл `~/.kube/config`, а также себе на локальную машину для внешнего подключения к кластеру.  
 Изменим владельца файла на текущего пользователя.
 На локальной машине в файле `~/.kube/config` изменим адрес сервера на внешний.  
 Проверяем:  
 ![Скриншот 5](img/5.png)  

 ### Создание тестового приложения
 Репозитарий с тестовым приложением находится по ссылке https://github.com/bodra84/mysite  
 Приложение состоит из веб сервера nginx, который отдает статическую страничку index.html.  
 ![Скриншот 6](img/6.png)  
 Создаем образ из докерфайла, запускаем контейнер, проверяем страничку, пушим образ на DockerHub.
![Скриншот 7](img/7.png)  
![Скриншот 8](img/8.png)  
![Скриншот 9](img/9.png)  

### Подготовка cистемы мониторинга и деплой приложения
1. Для деплоя в кластер системы мониторинга воспользуемся helm чартом kube-prometheus-stack из репозитария [prometheus-community](https://github.com/prometheus-community).  
Добавим репозиторий командой:  
`helm repo add prometheus-community https://prometheus-community.github.io/helm-charts`  
![Скриншот 10](img/10.png)  
Обновим репозитарии:  
`helm repo update`  
Сохраним файл конфигурации:  
`helm show values prometheus-community/kube-prometheus-stack > values.yml`  
![Скриншот 11](img/11.png)  
В файле изменим пароль администратора и параметры сервиса grafana.  
![Скриншот 12](img/12.png)  
![Скриншот 13](img/13.png)  
Файл конфигурации находится по ссылке [values.yml](helm/values.yml)  

Далее установим чарт применив файл конфигурации.  
![Скриншот 14](img/14.png)   
Проверим:  
![Скриншот 15](img/15.png)  

Для доступа к web интерфейсу grafana и к приложению mysite на 80 порту, а также для балансировки нагрузки между нодами, создадим два сетевых балансировщика и подключим к ним таргет группы, состоящие из наших нод.  
Код для создания данных ресурсов добавлен в файл main.tf в папке [infra](./terraform/infra/). 
При отправке кода в удаленный репозитарий запустится workflow GitHub Actions, который запустит процесс CI/CD и создаст в Яндекс клауд необходимые ресурсы.
Чтобы workflow отработал в GitHub были добавлены необходимые переменные в secrets.  
![Скриншот 16](img/16.png)  

Также пайплайном предусмотрен ручное применения конфигурации terraform, а также ее удаление при необходимости.  
![Скриншот 17](img/17.png)  

Процесс применения конфигурации terraform представлен на скриншоте:  
![Скриншот 18](img/18.png)  

Пайплайн в процессе своей работы сохраняет в артефакты терраформ план.
